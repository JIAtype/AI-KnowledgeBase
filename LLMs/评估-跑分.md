[AI Model Evaluations](https://artificialanalysis.ai/evaluations)
各个测试题（Benchmark/测试集）

### [HLE](https://agi.safe.ai/) (humanitie’s last exam) 
Humanity’s Last Exam is a challenging evaluation benchmark designed to test the reasoning, general knowledge, and world understanding capabilities of large language models. Unlike conventional benchmarks that focus on specific domains or straightforward question types, Humanity’s Last Exam features highly complex, open-ended questions covering philosophy, logic, ethics, science, and more. The benchmark is intended to probe the upper limits of an AI’s reasoning—posing questions that require not only factual knowledge but also deep comprehension, critical thinking, and sometimes even meta-cognitive skills. It has become a reference point for assessing whether advanced language models can approach or surpass human-level judgment on multifaceted, humanistic problems.
一项极具挑战性的评估基准测试集，旨在测试大型语言模型的推理能力、常识和世界理解能力。与专注于特定领域或简单问题类型的传统基准测试集不同，人文的最后考试集包含高度复杂、开放式的问题，涵盖哲学、逻辑、伦理、科学等领域。该基准旨在探索人工智能推理能力的上限——提出的问题不仅需要事实知识，还需要深度理解、批判性思维，有时甚至需要元认知技能。它已成为评估高级语言模型在处理多层面人文问题时能否接近或超越人类判断水平的参考点。

### [MMLU](https://github.com/hendrycks/test) (Massive Multitask Language Understanding)
MMLU is a comprehensive benchmark designed to evaluate a model's multitask language understanding abilities. It consists of over 10,000 multiple-choice questions across 57 disciplines, including mathematics, history, law, and medicine. MMLU is widely used to gauge the general knowledge, reasoning, and problem-solving abilities of large language models (LLMs).
全面的基准测试，旨在评估模型的多任务语言理解能力。它包含 57 个学科的 10,000 多个多项选择题，涵盖数学、历史、法律和医学等。MMLU 被广泛用于衡量大型语言模型 (LLM) 的常识、推理和问题解决能力。

### [C-Eval](https://cevalbenchmark.com/static/leaderboard.html)
C-Eval is a Chinese-centric comprehensive evaluation benchmark for large language models, focusing on testing linguistic and task abilities in the Chinese language. C-Eval covers multiple subject areas, including humanities, social sciences, STEM, and interdisciplinary topics, making it important for assessing models' capabilities in Chinese language understanding and reasoning.
以中文为中心的大型语言模型综合评估基准测试，专注于测试中文的语言能力和任务能力。C-Eval 涵盖多个学科领域，包括人文、社会科学、STEM 和跨学科主题，对于评估模型的中文理解和推理能力至关重要。

### [superCLUE](https://www.superclueai.com/)
superCLUE is a large-scale Chinese benchmark that extends the CLUE (Chinese Language Understanding Evaluation) suite. Aimed at both general and advanced abilities, superCLUE includes diverse and challenging tasks such as reading comprehension, natural language inference, and open-ended generation, allowing for thorough evaluation of LLMs' Chinese language skills.
一个大规模中文基准测试，扩展了 CLUE（中文语言理解评估）套件。 superCLUE 面向通用能力和高级能力，包含阅读理解、自然语言推理和开放式生成等多样化且具有挑战性的任务，能够全面评估法学硕士 (LLM) 的中文语言能力。

### [GPQA](https://artificialanalysis.ai/evaluations/gpqa-diamond) (Graduate-level Google Proof Physics QA Benchmark)
GPQA is a benchmark dataset of high-difficulty, graduate-level multiple-choice questions in physics. It evaluates a language model's deep understanding and reasoning in advanced physics topics, serving as a rigorous test for domain-specific knowledge.
一个高难度、研究生级物理学多项选择题的基准数据集。它评估语言模型对高级物理学主题的深度理解和推理能力，是对特定领域知识的严格测试。448 道选择题

### AIME25 (American Invitational Mathematics Examination)
AIME25 is a problem set composed of highly challenging math questions from the American Invitational Mathematics Examination, specifically the 25-question version. This benchmark assesses a model's mathematical problem-solving and reasoning ability at an advanced competition level.
AIME25 是一个由美国数学邀请赛（特别是 25 道题的版本）中极具挑战性的数学题目组成的问题集。该基准测试以高级竞赛水平评估模型的数学问题解决和推理能力。

### LiveCodeBench
LiveCodeBench is an evaluation benchmark designed to test a model's live coding capabilities. It contains tasks that require the model to write, debug, and explain code in a variety of programming languages, emphasizing both correctness and the ability to handle real-time coding challenges.
一个评估基准测试，旨在测试模型的实时编码能力。它包含要求模型使用多种编程语言编写、调试和解释代码的任务，重点考察模型的正确性和处理实时编码挑战的能力。

## Aider
Aider is an open-source benchmark designed to evaluate code understanding and code generation abilities of large language models (LLMs). The benchmark consists of a diverse set of programming tasks, including real-world coding, code completion, code modification, and explanation problems, across multiple languages and difficulty levels. Aider aims to assess the practical and comprehensive programming skills of LLMs, measuring their capabilities in not just generating code, but also understanding and editing existing code bases effectively.
一个开源基准测试，旨在评估大型语言模型 (LLM) 的代码理解和代码生成能力。该基准测试包含一系列丰富的编程任务，包括真实世界编码、代码补全、代码修改和解释问题，涵盖多种语言和难度级别。Aider 旨在评估 LLM 的实用和综合编程技能，不仅衡量其生成代码的能力，还衡量其有效理解和编辑现有代码库的能力。

### ArenaHard
ArenaHard is an advanced, adversarial benchmark designed to evaluate the robustness and higher-level reasoning capabilities of language models. It contains carefully constructed, difficult samples across a range of domains to stress-test and compare strong models in a head-to-head ("arena") fashion.
一个先进的对抗性基准测试，旨在评估语言模型的鲁棒性和高级推理能力。它包含精心构建的、涵盖多个领域的高难度样本，用于对强模型进行压力测试，并以“竞技场”的方式进行正面交锋。

### BFCL (Bilingual Few-shot Chain-of-thought Learning)
BFCL is a bilingual benchmark for evaluating large language models' few-shot reasoning capacities in both Chinese and English. It focuses on "chain-of-thought" multi-step reasoning problems across multiple disciplines, reflecting a model's ability to perform complex reasoning in a bilingual setting.
一个双语基准测试集，用于评估大型语言模型在中文和英文环境下的小样本推理能力。它专注于跨学科的“思维链”多步骤推理问题，反映模型在双语环境下进行复杂推理的能力。